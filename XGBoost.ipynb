{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "493c3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "machine = pd.read_csv(r\"C:\\Users\\CalumBrown\\OneDrive - Blend 360\\Documents\\Personal Development\\Machine Learning Interview\\Cleaned Machine Failure Dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba5aa32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = machine[['Air temperature [K]', 'Process temperature [K]',\n",
    "       'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]\n",
    "# Feature names cant contain symbols for xgboost model\n",
    "x = x.rename(columns = {'Air temperature [K]':'Air temperature', 'Process temperature [K]':'Process temperature','Rotational speed [rpm]': 'Rotational speed', 'Torque [Nm]':'Torque', 'Tool wear [min]': 'Tool wear'})\n",
    "y = machine['Failure Type']\n",
    "y2 = machine['Failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e057388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model on failure type\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size = 0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbf1a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding failure type column after split to avoid data leakage\n",
    "# Encoding our target variable\n",
    "# Should do this after split to avoid data leakage\n",
    "Label_Encoder = LabelEncoder()\n",
    "ytrain = Label_Encoder.fit_transform(ytrain)\n",
    "ytest = Label_Encoder.fit_transform(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "764ddb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting model \n",
    "xgb1 = xgb.XGBClassifier(random_state = 42)\n",
    "xgb1.fit(xtrain, ytrain)\n",
    "xgb1_predict = xgb1.predict(xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80a4f40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9819789187351241\n",
      "[[  29    3    0    1    0    0]\n",
      " [   0 2831    4    6    0    3]\n",
      " [   0    6   13    2    0    0]\n",
      " [   1   13    0   15    0    0]\n",
      " [   0    5    0    0    0    0]\n",
      " [   0    9    0    0    0    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92        33\n",
      "           1       0.99      1.00      0.99      2844\n",
      "           2       0.76      0.62      0.68        21\n",
      "           3       0.62      0.52      0.57        29\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.98      2941\n",
      "   macro avg       0.56      0.50      0.53      2941\n",
      "weighted avg       0.98      0.98      0.98      2941\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CalumBrown\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\CalumBrown\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\CalumBrown\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## Checking accuracy score and metrics\n",
    "\n",
    "print(accuracy_score(ytest, xgb1_predict))\n",
    "print(confusion_matrix(ytest, xgb1_predict))\n",
    "print(classification_report(ytest, xgb1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c5c1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning \n",
    "# It is our job to set parameters which can influence the model fit\n",
    "# These are different paramaters and theory is quite complex but all impact how model works\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.7, 0.85, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.85, 1.0],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1, 10, 100],\n",
    "    'reg_lambda': [0.5, 0.7, 1, 1.3]\n",
    "}\n",
    "\n",
    "# We then use cross validation to test different combinations, loop through each one of these validation folds\n",
    "# Can either loop through every combination (gridcv) or random (randomcv)\n",
    "#This uses its own cross validation system so no need for validation split as just lose important training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ceffdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "Best Parameters: {'subsample': 1.0, 'reg_lambda': 0.5, 'reg_alpha': 1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 9, 'learning_rate': 0.1, 'colsample_bytree': 0.85}\n",
      "Best Accuracy: 0.9785714285714284\n",
      "0.9816388983339001\n",
      "[[  30    2    0    1    0    0]\n",
      " [   1 2835    3    5    0    0]\n",
      " [   0    8   11    2    0    0]\n",
      " [   1   16    1   11    0    0]\n",
      " [   0    5    0    0    0    0]\n",
      " [   0    9    0    0    0    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92        33\n",
      "           1       0.99      1.00      0.99      2844\n",
      "           2       0.73      0.52      0.61        21\n",
      "           3       0.58      0.38      0.46        29\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.98      2941\n",
      "   macro avg       0.54      0.47      0.50      2941\n",
      "weighted avg       0.98      0.98      0.98      2941\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CalumBrown\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\CalumBrown\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\CalumBrown\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# fitting model testing different params \n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "grid_search = RandomizedSearchCV(xgb_model, param_grid, cv=10, scoring=\"accuracy\", n_iter=100, n_jobs=-1, verbose=2, random_state=42)\n",
    "grid_search.fit(xtrain, ytrain)\n",
    "\n",
    "#Returns model with best score - is this what you always want?\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Best parameters from tuning\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "xgbparams_predict = best_xgb.predict(xtest)\n",
    "\n",
    "print(accuracy_score(ytest, xgbparams_predict))\n",
    "print(confusion_matrix(ytest, xgbparams_predict))\n",
    "print(classification_report(ytest, xgbparams_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a01de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing binary machine failure with xgboost\n",
    "\n",
    "x2train, x2test, y2train, y2test = train_test_split(x,y2,test_size = 0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e26edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9802788167290037\n",
      "[[2825   19]\n",
      " [  39   58]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2844\n",
      "           1       0.75      0.60      0.67        97\n",
      "\n",
      "    accuracy                           0.98      2941\n",
      "   macro avg       0.87      0.80      0.83      2941\n",
      "weighted avg       0.98      0.98      0.98      2941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting model with no fine tuning\n",
    "xgbfail = xgb.XGBClassifier(random_state = 42)\n",
    "xgbfail.fit(x2train, y2train)\n",
    "xgbfail_predict = xgbfail.predict(x2test)\n",
    "\n",
    "print(accuracy_score(y2test, xgbfail_predict))\n",
    "print(confusion_matrix(y2test, xgbfail_predict))\n",
    "print(classification_report(y2test, xgbfail_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c150bbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "Best Parameters: {'subsample': 1.0, 'reg_lambda': 0.5, 'reg_alpha': 1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "Best Accuracy: 0.9774052478134113\n",
      "0.9816388983339001\n",
      "[[2827   17]\n",
      " [  37   60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2844\n",
      "           1       0.78      0.62      0.69        97\n",
      "\n",
      "    accuracy                           0.98      2941\n",
      "   macro avg       0.88      0.81      0.84      2941\n",
      "weighted avg       0.98      0.98      0.98      2941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter tuning xgboost model\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.7, 0.85, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.85, 1.0],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1, 10, 100],\n",
    "    'reg_lambda': [0.5, 0.7, 1, 1.3]\n",
    "}\n",
    "\n",
    "xgb_modelfail = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "grid_search = RandomizedSearchCV(xgb_modelfail, param_grid, cv=10, scoring=\"accuracy\", n_iter=100, n_jobs=-1, verbose=2, random_state=42)\n",
    "grid_search.fit(x2train, y2train)\n",
    "\n",
    "#Returns model with best score - is this what you always want?\n",
    "best_xgbfail = grid_search.best_estimator_\n",
    "\n",
    "# Best parameters from tuning\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "xgbparamsfail_predict = best_xgbfail.predict(x2test)\n",
    "\n",
    "print(accuracy_score(y2test, xgbparamsfail_predict))\n",
    "print(confusion_matrix(y2test, xgbparamsfail_predict))\n",
    "print(classification_report(y2test, xgbparamsfail_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c7792da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "Best Parameters: {'subsample': 1.0, 'reg_lambda': 0.7, 'reg_alpha': 0.01, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "Best Accuracy: 0.7695285710259258\n",
      "0.9806188371302278\n",
      "[[2825   19]\n",
      " [  38   59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2844\n",
      "           1       0.76      0.61      0.67        97\n",
      "\n",
      "    accuracy                           0.98      2941\n",
      "   macro avg       0.87      0.80      0.83      2941\n",
      "weighted avg       0.98      0.98      0.98      2941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trying with recall_macro as evaluation optimal score\n",
    "\n",
    "xgb_modelfail = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "grid_search = RandomizedSearchCV(xgb_modelfail, param_grid, cv=10, scoring=\"recall_macro\", n_iter=100, n_jobs=-1, verbose=2, random_state=42)\n",
    "grid_search.fit(x2train, y2train)\n",
    "\n",
    "#Returns model with best score - is this what you always want?\n",
    "best_xgbfail = grid_search.best_estimator_\n",
    "\n",
    "# Best parameters from tuning\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "xgbparamsfail_predict = best_xgbfail.predict(xtest)\n",
    "\n",
    "print(accuracy_score(y2test, xgbparamsfail_predict))\n",
    "print(confusion_matrix(y2test, xgbparamsfail_predict))\n",
    "print(classification_report(y2test, xgbparamsfail_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4beb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "Best Parameters: {'subsample': 1.0, 'reg_lambda': 0.5, 'reg_alpha': 1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "Best Accuracy: 0.9774052478134113\n",
      "0.981298877932676\n",
      "[[2821   23]\n",
      " [  32   65]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2844\n",
      "           1       0.74      0.67      0.70        97\n",
      "\n",
      "    accuracy                           0.98      2941\n",
      "   macro avg       0.86      0.83      0.85      2941\n",
      "weighted avg       0.98      0.98      0.98      2941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping classification for failure threshold down to 0.4\n",
    "\n",
    "xgb_modelfail = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "grid_search = RandomizedSearchCV(xgb_modelfail, param_grid, cv=10, scoring=\"accuracy\", n_iter=100, n_jobs=-1, verbose=2, random_state=42)\n",
    "grid_search.fit(x2train, y2train)\n",
    "\n",
    "#Returns model with best score - is this what you always want?\n",
    "best_xgbfail = grid_search.best_estimator_\n",
    "\n",
    "# Best parameters from tuning\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "probs = best_xgbfail.predict_proba(x2test)[:, 1]  # probability of class 1\n",
    "\n",
    "# Set a lower threshold, e.g., 0.3 or 0.4 instead of 0.5\n",
    "threshold = 0.4\n",
    "rffail1_test = (probs >= threshold).astype(int)\n",
    "\n",
    "\n",
    "print(accuracy_score(y2test, rffail1_test))\n",
    "print(confusion_matrix(y2test, rffail1_test))\n",
    "print(classification_report(y2test, rffail1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd519b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
